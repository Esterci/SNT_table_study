{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47598 entries, 0 to 47597\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   uf               47598 non-null  object \n",
      " 1   sexo             47598 non-null  object \n",
      " 2   grupo_sanguineo  47598 non-null  object \n",
      " 3   cor              47598 non-null  object \n",
      " 4   uf_origem        47598 non-null  object \n",
      " 5   tipo_de_doador   38331 non-null  object \n",
      " 6   idade            47598 non-null  float64\n",
      " 7   obito_bin        47598 non-null  int64  \n",
      " 8   transplante_bin  47598 non-null  int64  \n",
      " 9   delta_t          47598 non-null  float64\n",
      "dtypes: float64(2), int64(2), object(6)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "file = \"../estudo_inicial/bases_de_dados/base_de_dados_formatada_rim.csv\"\n",
    "\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=[\"regiao\", \"data_da_inscricao\", \"data_do_evento\"])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47598 entries, 0 to 47597\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   uf               47598 non-null  object \n",
      " 1   sexo             47598 non-null  object \n",
      " 2   grupo_sanguineo  47598 non-null  object \n",
      " 3   cor              47598 non-null  object \n",
      " 4   uf_origem        47598 non-null  object \n",
      " 5   idade            47598 non-null  float64\n",
      " 6   obito_bin        47598 non-null  int64  \n",
      " 7   transplante_bin  47598 non-null  int64  \n",
      " 8   delta_t          47598 non-null  float64\n",
      "dtypes: float64(2), int64(2), object(5)\n",
      "memory usage: 3.3+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15297/3844140227.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.transplante_bin.loc[df.tipo_de_doador == 0] = 0\n",
      "/tmp/ipykernel_15297/3844140227.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.transplante_bin.loc[df.tipo_de_doador != 0] = 1\n"
     ]
    }
   ],
   "source": [
    "df = df.fillna(0)\n",
    "df.transplante_bin.loc[df.tipo_de_doador == 0] = 0\n",
    "df.transplante_bin.loc[df.tipo_de_doador != 0] = 1\n",
    "df = df.drop(columns=['tipo_de_doador'])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([ 9267, 38331]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df.transplante_bin,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47598 entries, 0 to 47597\n",
      "Data columns (total 65 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   idade               47598 non-null  float64\n",
      " 1   obito_bin           47598 non-null  int64  \n",
      " 2   transplante_bin     47598 non-null  int64  \n",
      " 3   uf_AC               47598 non-null  int64  \n",
      " 4   uf_AL               47598 non-null  int64  \n",
      " 5   uf_AM               47598 non-null  int64  \n",
      " 6   uf_BA               47598 non-null  int64  \n",
      " 7   uf_CE               47598 non-null  int64  \n",
      " 8   uf_DF               47598 non-null  int64  \n",
      " 9   uf_ES               47598 non-null  int64  \n",
      " 10  uf_GO               47598 non-null  int64  \n",
      " 11  uf_MA               47598 non-null  int64  \n",
      " 12  uf_MG               47598 non-null  int64  \n",
      " 13  uf_MS               47598 non-null  int64  \n",
      " 14  uf_MT               47598 non-null  int64  \n",
      " 15  uf_PA               47598 non-null  int64  \n",
      " 16  uf_PB               47598 non-null  int64  \n",
      " 17  uf_PE               47598 non-null  int64  \n",
      " 18  uf_PI               47598 non-null  int64  \n",
      " 19  uf_PR               47598 non-null  int64  \n",
      " 20  uf_RJ               47598 non-null  int64  \n",
      " 21  uf_RN               47598 non-null  int64  \n",
      " 22  uf_RO               47598 non-null  int64  \n",
      " 23  uf_RS               47598 non-null  int64  \n",
      " 24  uf_SC               47598 non-null  int64  \n",
      " 25  uf_SE               47598 non-null  int64  \n",
      " 26  uf_SP               47598 non-null  int64  \n",
      " 27  uf_origem_AC        47598 non-null  int64  \n",
      " 28  uf_origem_AL        47598 non-null  int64  \n",
      " 29  uf_origem_AM        47598 non-null  int64  \n",
      " 30  uf_origem_AP        47598 non-null  int64  \n",
      " 31  uf_origem_BA        47598 non-null  int64  \n",
      " 32  uf_origem_CE        47598 non-null  int64  \n",
      " 33  uf_origem_DF        47598 non-null  int64  \n",
      " 34  uf_origem_ES        47598 non-null  int64  \n",
      " 35  uf_origem_GO        47598 non-null  int64  \n",
      " 36  uf_origem_MA        47598 non-null  int64  \n",
      " 37  uf_origem_MG        47598 non-null  int64  \n",
      " 38  uf_origem_MS        47598 non-null  int64  \n",
      " 39  uf_origem_MT        47598 non-null  int64  \n",
      " 40  uf_origem_PA        47598 non-null  int64  \n",
      " 41  uf_origem_PB        47598 non-null  int64  \n",
      " 42  uf_origem_PE        47598 non-null  int64  \n",
      " 43  uf_origem_PI        47598 non-null  int64  \n",
      " 44  uf_origem_PR        47598 non-null  int64  \n",
      " 45  uf_origem_RJ        47598 non-null  int64  \n",
      " 46  uf_origem_RN        47598 non-null  int64  \n",
      " 47  uf_origem_RO        47598 non-null  int64  \n",
      " 48  uf_origem_RR        47598 non-null  int64  \n",
      " 49  uf_origem_RS        47598 non-null  int64  \n",
      " 50  uf_origem_SC        47598 non-null  int64  \n",
      " 51  uf_origem_SE        47598 non-null  int64  \n",
      " 52  uf_origem_SP        47598 non-null  int64  \n",
      " 53  uf_origem_TO        47598 non-null  int64  \n",
      " 54  sexo_F              47598 non-null  int64  \n",
      " 55  sexo_M              47598 non-null  int64  \n",
      " 56  grupo_sanguineo_A   47598 non-null  int64  \n",
      " 57  grupo_sanguineo_AB  47598 non-null  int64  \n",
      " 58  grupo_sanguineo_B   47598 non-null  int64  \n",
      " 59  grupo_sanguineo_O   47598 non-null  int64  \n",
      " 60  cor_Amarela         47598 non-null  int64  \n",
      " 61  cor_Branca          47598 non-null  int64  \n",
      " 62  cor_Indigena        47598 non-null  int64  \n",
      " 63  cor_Negra           47598 non-null  int64  \n",
      " 64  cor_Parda           47598 non-null  int64  \n",
      "dtypes: float64(1), int64(64)\n",
      "memory usage: 23.6 MB\n"
     ]
    }
   ],
   "source": [
    "df_bin = df\n",
    "\n",
    "bin_cols = [\n",
    "    \"uf\",\n",
    "    # \"tipo_de_doador\",\n",
    "    \"uf_origem\",\n",
    "    \"sexo\",\n",
    "    \"grupo_sanguineo\",\n",
    "    \"cor\",\n",
    "]\n",
    "\n",
    "for i,col in enumerate(bin_cols):\n",
    "    # converting to binary data\n",
    "    df_one = pd.get_dummies(\n",
    "        df[col],\n",
    "        dtype=int\n",
    "    )\n",
    "\n",
    "\n",
    "    # Renomeando colunas\n",
    "    new_columns = [\n",
    "        col + \"_\" + category.strip().replace(\" \", \"_\") for category in df_one.columns\n",
    "    ]\n",
    "\n",
    "    df_one.columns = new_columns\n",
    "\n",
    "    df_bin = pd.concat([df_bin, df_one], axis=1)\n",
    "\n",
    "    df_bin = df_bin.drop(columns=col)\n",
    "\n",
    "df_bin = df_bin.drop(columns=[\"delta_t\"])\n",
    "\n",
    "df_bin.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle as pkl\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "norm_df = df_bin\n",
    "\n",
    "norm_df.idade = scaler.fit_transform(norm_df.idade.values.reshape((-1, 1)))\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "X = norm_df.drop(columns=[\"transplante_bin\", \"obito_bin\"])\n",
    "y = norm_df.drop(columns=[\"transplante_bin\", \"obito_bin\"]).values\n",
    "y_real = norm_df.transplante_bin.values\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    fold_dict = {\n",
    "        \"X_train\": X.values[train_index, :],\n",
    "        \"X_test\": X.values[test_index, :],\n",
    "        \"y_train\": y[train_index],\n",
    "        \"y_test\": y[test_index],\n",
    "        \"y_real_train\": y_real[train_index],\n",
    "        \"y_real_test\": y_real[test_index],\n",
    "    }\n",
    "\n",
    "    with open(\"folds/fold_{}.pkl\".format(i), \"wb\") as f:\n",
    "        pkl.dump(fold_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN model creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 00:07:40.039657: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel(\"INFO\")\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from keras import backend as K\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_builder:\n",
    "    @classmethod\n",
    "    def add_layer(cls, neurons, ac_function, mlp):\n",
    "        mlp = tf.keras.layers.Dense(units=neurons, activation=ac_function)(mlp)\n",
    "        mlp = tf.keras.layers.Dropout(0.2)(mlp)\n",
    "        return mlp\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers,\n",
    "        ac_function,\n",
    "        input_sequence_length,\n",
    "        batch,\n",
    "        iterr,\n",
    "        learningRate,\n",
    "        beta1,\n",
    "        beta2,\n",
    "        epocas,\n",
    "        output_function=\"linear\",\n",
    "        shuffle=True,\n",
    "    ):\n",
    "        self.layers = layers\n",
    "        self.ac_function = ac_function\n",
    "        self.input_sequence_length = input_sequence_length\n",
    "        self.batch = batch\n",
    "        self.iterr = iterr\n",
    "        self.learningRate = learningRate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epocas = epocas\n",
    "        self.shuffle = shuffle\n",
    "        self.output_function = output_function\n",
    "\n",
    "    def fit_predict(self, X_train, y_train, X_test, struct_name, val_split=0.1):\n",
    "        # print(input_sequence_length)\n",
    "        input_layer = tf.keras.layers.Input(shape=(self.input_sequence_length,))\n",
    "\n",
    "        mlp = input_layer  # Inicializar o modelo com a camada de entrada\n",
    "\n",
    "        # Construindo encoder\n",
    "        for neurons in self.layers:\n",
    "            self.add_layer(neurons, self.ac_function, mlp)\n",
    "\n",
    "        # Camada encodada\n",
    "        mlp = self.add_layer(1, self.output_function, mlp)\n",
    "\n",
    "        # Contruindo decoder\n",
    "        for neurons in self.layers:\n",
    "            self.add_layer(neurons, self.ac_function, mlp)\n",
    "\n",
    "        # Camada de saída com output_dim=1\n",
    "\n",
    "        output_function = tf.keras.layers.Dense(\n",
    "            self.input_sequence_length, activation=\"softmax\"\n",
    "        )(mlp)\n",
    "\n",
    "        # Definir o modelo\n",
    "        mlp_model = tf.keras.Model(inputs=input_layer, outputs=output_function)\n",
    "\n",
    "        opt = tf.keras.optimizers.legacy.Adam(\n",
    "            learning_rate=self.learningRate, beta_1=self.beta1, beta_2=self.beta2\n",
    "        )\n",
    "\n",
    "        # Compilar o modelo\n",
    "        mlp_model.compile(loss=\"mean_squared_error\", optimizer=opt)\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=val_split, shuffle=self.shuffle, stratify=None\n",
    "        )\n",
    "\n",
    "        early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=self.iterr,\n",
    "            verbose=0,\n",
    "            min_delta=0.0001,\n",
    "            mode=\"min\",\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "\n",
    "        start_fit_time = time.time()\n",
    "\n",
    "        history = mlp_model.fit(\n",
    "            x=X_train,\n",
    "            y=y_train,\n",
    "            batch_size=int(self.batch),\n",
    "            epochs=self.epocas,\n",
    "            verbose=0,\n",
    "            callbacks=[early_stopping_callback],\n",
    "            # validation_split=0.0,\n",
    "            validation_data=(X_val, y_val),\n",
    "            shuffle=self.shuffle,\n",
    "            workers=10,\n",
    "            use_multiprocessing=True,\n",
    "        ).history\n",
    "\n",
    "        end_fit_time = time.time()\n",
    "\n",
    "        fit_time = end_fit_time - start_fit_time\n",
    "\n",
    "        # Ploting Model Loss\n",
    "\n",
    "        plt.ioff()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(history[\"loss\"], linewidth=2, label=\"Train\")\n",
    "        plt.plot(history[\"val_loss\"], linewidth=2, label=\"Validation\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Model loss\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "\n",
    "        # plt.show(block=)\n",
    "\n",
    "        fig.savefig(\n",
    "            \"learning_curves/model-loss__\" + struct_name + \"__.png\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        del fig\n",
    "\n",
    "        start_predict_time = time.time()\n",
    "\n",
    "        output = mlp_model.predict(X_test, workers=10, use_multiprocessing=True)\n",
    "\n",
    "        end_predict_time = time.time()\n",
    "\n",
    "        predict_time = end_predict_time - start_predict_time\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "        return output, fit_time, predict_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31024, 63)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30664, 63)\n",
      "(30635, 63)\n",
      "(30501, 63)\n",
      "(30500, 63)\n",
      "(31024, 63)\n",
      "(30664, 63)\n",
      "(30635, 63)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "hidden_layer_sizes = [\n",
    "    (3, 3),\n",
    "    # (5, 5),\n",
    "    # (10, 10),\n",
    "    # (3, 3, 3),\n",
    "    # (5, 5, 5),\n",
    "    # (10, 10, 10),\n",
    "]\n",
    "activation = [\n",
    "    # \"softmax\",\n",
    "    \"sigmoid\",\n",
    "     \"tanh\",\n",
    "     \"relu\",\n",
    "     \"linear\",\n",
    "    # \"leaky_relu\",\n",
    "    # \"softplus\",\n",
    "    # \"softsign\",\n",
    "    # \"elu\",\n",
    "]\n",
    "\n",
    "output_function_list = [\n",
    "    # \"softmax\",\n",
    "    # \"sigmoid\",\n",
    "    # \"relu\",\n",
    "    \"linear\",\n",
    "    # \"tanh\",\n",
    "    # \"leaky_relu\",\n",
    "    # \"softplus\",\n",
    "    # \"softsign\",\n",
    "    # \"elu\",\n",
    "]\n",
    "\n",
    "batch_size = [\n",
    "    # 200,\n",
    "    300,\n",
    "    # 400\n",
    "]\n",
    "n_iter_no_change = [10]\n",
    "learningRates = [\n",
    "    # 0.001,\n",
    "    0.0001,\n",
    "    # 0.00001,\n",
    "    # 0.001,\n",
    "    # 0.01,\n",
    "]\n",
    "beta_1_List = [\n",
    "    # 0.99,\n",
    "    0.9,\n",
    "    # 0.85,\n",
    "]\n",
    "beta_2_List = [\n",
    "    0.99,\n",
    "    # 0.9,\n",
    "    # 0.85,\n",
    "]\n",
    "\n",
    "results_list = glob.glob(\"grid_results/*\")\n",
    "folds = glob.glob(\"folds/*\")\n",
    "\n",
    "\n",
    "for layers in hidden_layer_sizes:\n",
    "    for function in activation:\n",
    "        for batch in batch_size:\n",
    "            for iterr in n_iter_no_change:\n",
    "                for learningRate in learningRates:\n",
    "                    for beta1 in beta_1_List:\n",
    "                        for beta2 in beta_2_List:\n",
    "                            for output_function in output_function_list:\n",
    "                                hp_str = \"grid_results/auto_sem_tp_doador_layers_{}__activation_function_{}__output_function_{}__batch_{}__iter_{}__LearnR_{}__Beta1_{}__Beta2_{}.pkl\".format(\n",
    "                                    layers,\n",
    "                                    function,\n",
    "                                    output_function,\n",
    "                                    batch,\n",
    "                                    iterr,\n",
    "                                    learningRate,\n",
    "                                    beta1,\n",
    "                                    beta2,\n",
    "                                )\n",
    "\n",
    "                                if learningRate < 0.0001:\n",
    "                                    epocas = 100\n",
    "\n",
    "                                else:\n",
    "                                    epocas = 70\n",
    "\n",
    "                                if not hp_str in results_list:\n",
    "                                    # try:\n",
    "                                    global_fit_time = []\n",
    "                                    global_predict_time = []\n",
    "                                    model = MLP_builder(\n",
    "                                        layers,\n",
    "                                        function,\n",
    "                                        63,\n",
    "                                        batch,\n",
    "                                        iterr,\n",
    "                                        learningRate,\n",
    "                                        beta1,\n",
    "                                        beta2,\n",
    "                                        epocas=epocas,\n",
    "                                        output_function=output_function,\n",
    "                                    )\n",
    "\n",
    "                                    for i, fold in enumerate(folds):\n",
    "                                        with open(fold, \"rb\") as f:\n",
    "                                            fold_dict = pickle.load(f)\n",
    "\n",
    "                                        X_train = fold_dict[\"X_train\"]\n",
    "                                        y_train = fold_dict[\"y_train\"]\n",
    "                                        y_real_train = fold_dict[\"y_real_train\"]\n",
    "                                        X_test = fold_dict[\"X_test\"]\n",
    "                                        y_test = fold_dict[\"y_test\"]\n",
    "                                        print(X_train[y_real_train == 1].shape)\n",
    "                                        (\n",
    "                                            output,\n",
    "                                            fit_time,\n",
    "                                            predict_time,\n",
    "                                        ) = model.fit_predict(\n",
    "                                            X_train[y_real_train == 1, :],\n",
    "                                            y_train[y_real_train == 1],\n",
    "                                            X_test,\n",
    "                                            (\n",
    "                                                hp_str.split(\"/\")[-1]\n",
    "                                                + \"__fold_{}\".format(i)\n",
    "                                            ),\n",
    "                                        )\n",
    "\n",
    "                                        global_fit_time.append(fit_time)\n",
    "                                        global_predict_time.append(predict_time)\n",
    "\n",
    "                                        if i == 0:\n",
    "                                            global_output = output\n",
    "                                            global_target = y_test\n",
    "\n",
    "                                        else:\n",
    "                                            global_output = np.vstack(\n",
    "                                                (global_output, output)\n",
    "                                            )\n",
    "                                            global_target = np.vstack(\n",
    "                                                (global_target, y_test)\n",
    "                                            )\n",
    "\n",
    "                                    result = {\n",
    "                                        \"y_predict\": global_output,\n",
    "                                        \"y_test\": global_target,\n",
    "                                        \"fit_time\": [np.mean(global_fit_time)],\n",
    "                                        \"predict_time\": [np.mean(global_predict_time)],\n",
    "                                    }\n",
    "\n",
    "                                    with open(hp_str, \"wb\") as handle:\n",
    "                                        pkl.dump(result, handle)\n",
    "\n",
    "                                    # except:\n",
    "                                    #     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/home/thiago-esterci/Documents/GitHub/SNT_table_study/machine_learning/pipeline_prototype_class.ipynb Célula 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/thiago-esterci/Documents/GitHub/SNT_table_study/machine_learning/pipeline_prototype_class.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, file \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(results_list):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/thiago-esterci/Documents/GitHub/SNT_table_study/machine_learning/pipeline_prototype_class.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/thiago-esterci/Documents/GitHub/SNT_table_study/machine_learning/pipeline_prototype_class.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         df_result \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(file)\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnnamed: 0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/thiago-esterci/Documents/GitHub/SNT_table_study/machine_learning/pipeline_prototype_class.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         hp_serie \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m\"\u001b[39m\u001b[39mhyperparameters\u001b[39m\u001b[39m\"\u001b[39m: [file\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]})\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thiago-esterci/Documents/GitHub/SNT_table_study/machine_learning/pipeline_prototype_class.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         penalty_score \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thiago-esterci/Documents/GitHub/SNT_table_study/machine_learning/pipeline_prototype_class.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             {\u001b[39m\"\u001b[39m\u001b[39mpenalty_score\u001b[39m\u001b[39m\"\u001b[39m: df_result\u001b[39m.\u001b[39mrmse\u001b[39m.\u001b[39mvalues \u001b[39m+\u001b[39m df_result[\u001b[39m'\u001b[39m\u001b[39mstd\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues}\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thiago-esterci/Documents/GitHub/SNT_table_study/machine_learning/pipeline_prototype_class.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1722\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[1;32m   1724\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m kwds[\u001b[39m\"\u001b[39m\u001b[39mdtype_backend\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[39m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m parsers\u001b[39m.\u001b[39;49mTextReader(src, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munnamed_cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "results_list = glob.glob(\"grid_results/*\")\n",
    "\n",
    "df_result = False\n",
    "for i, file in enumerate(results_list):\n",
    "    if i == 0:\n",
    "        df_result = pd.read_csv(file).drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "        hp_serie = pd.DataFrame({\"hyperparameters\": [file.split(\"/\")[-1]]})\n",
    "\n",
    "        penalty_score = pd.DataFrame(\n",
    "            {\"penalty_score\": df_result.rmse.values + df_result['std'].values}\n",
    "        )\n",
    "\n",
    "        df_result = pd.concat(\n",
    "            (df_result, penalty_score,hp_serie), axis=1\n",
    "        )\n",
    "\n",
    "        df_result.index = [i]\n",
    "\n",
    "    else:\n",
    "        df_aux = pd.read_csv(file).drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "        hp_serie = pd.DataFrame({\"hyperparameters\": [file.split(\"/\")[-1]]})\n",
    "\n",
    "        penalty_score = pd.DataFrame(\n",
    "            {\"penalty_score\": df_aux.rmse.values + df_aux['std'].values}\n",
    "        )\n",
    "\n",
    "        df_aux = pd.concat(\n",
    "            (df_aux, penalty_score,hp_serie), axis=1\n",
    "        )\n",
    "        \n",
    "        df_aux.index = [i]\n",
    "\n",
    "        df_result = pd.concat((df_result, df_aux), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>std</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>predict_time</th>\n",
       "      <th>penalty_score</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>layers_(3, 3)__function_relu__batch_400__iter_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>layers_(10, 10)__function_relu__batch_200__ite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>layers_(5, 5, 5)__function_softmax__batch_400_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>layers_(3, 3, 3)__function_sigmoid__batch_200_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>layers_(5, 5, 5)__function_linear__batch_300__...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rmse  std  fit_time  predict_time  penalty_score  \\\n",
       "0        0    0       0.0           0.0              0   \n",
       "1086     0    0       0.0           0.0              0   \n",
       "1085     0    0       0.0           0.0              0   \n",
       "1084     0    0       0.0           0.0              0   \n",
       "1083     0    0       0.0           0.0              0   \n",
       "\n",
       "                                        hyperparameters  \n",
       "0     layers_(3, 3)__function_relu__batch_400__iter_...  \n",
       "1086  layers_(10, 10)__function_relu__batch_200__ite...  \n",
       "1085  layers_(5, 5, 5)__function_softmax__batch_400_...  \n",
       "1084  layers_(3, 3, 3)__function_sigmoid__batch_200_...  \n",
       "1083  layers_(5, 5, 5)__function_linear__batch_300__...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.sort_values(by=[\"penalty_score\"], ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snt_tables",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
