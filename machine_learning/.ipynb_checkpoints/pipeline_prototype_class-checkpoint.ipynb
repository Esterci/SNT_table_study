{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../estudo_inicial/bases_de_dados/base_de_dados_formatada_rim.csv\"\n",
    "\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(\n",
    "    columns=[\n",
    "        \"delta_t\",\n",
    "        \"data_de_nascimento\",\n",
    "        \"data_do_obito\",\n",
    "        \"data_do_transplante\",\n",
    "        \"tipo_de_doador\",\n",
    "        \"regiao\",\n",
    "        \"data_da_inscricao\",\n",
    "        \"data_do_evento\",\n",
    "    ]\n",
    ")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin = df\n",
    "\n",
    "bin_cols = [\n",
    "    \"uf\",\n",
    "    \"uf_origem\",\n",
    "    \"sexo\",\n",
    "    \"grupo_sanguineo\",\n",
    "    \"cor\",\n",
    "]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "for i,col in enumerate(bin_cols):\n",
    "\n",
    "    # integer encode\n",
    "    integer_encoded = label_encoder.fit_transform(df[col])\n",
    "\n",
    "    _ , index = np.unique(integer_encoded,return_index=True)\n",
    "\n",
    "    categories = [ df[col].values[i] for i in  index]\n",
    "\n",
    "    # binary encode\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "    # Renomeando colunas\n",
    "    new_columns = [\n",
    "        col + \"_\" + category.strip().replace(\" \", \"_\") for category in categories\n",
    "    ]\n",
    "\n",
    "    df_one = pd.DataFrame(onehot_encoded,columns = new_columns)\n",
    "\n",
    "    df_bin = pd.concat([df_bin, df_one], axis=1)\n",
    "\n",
    "    df_bin = df_bin.drop(columns=col)\n",
    "\n",
    "df_bin.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanceamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def balance(X,y,ids):\n",
    "\n",
    "    X_0 = X[y==0]\n",
    "    X_1 = X[y==1]\n",
    "\n",
    "    y_0 = y[y==0]\n",
    "    y_1 = y[y==1]\n",
    "    \n",
    "    ids_0 = ids[y==0]\n",
    "    ids_1 = ids[y==1]\n",
    "\n",
    "    n_obitos = len(X_0)\n",
    "\n",
    "    X_1_bal,_, y_1_bal,_,ids_1_bal,_ = train_test_split(\n",
    "            X_1, y_1 ,ids_1, train_size=n_obitos+53\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_bal = pd.DataFrame(np.vstack([X_0,X_1_bal]), columns=X.columns)\n",
    "    y_bal = np.hstack((y_0,y_1_bal))\n",
    "    ids_bal = np.hstack((ids_0,ids_1_bal))\n",
    "    \n",
    "    print(\"There are {} samples of label 0\".format(n_obitos))\n",
    "    print(\"There are {} samples of label 1\".format(len(X_1_bal)))\n",
    "    print(\"The number of balanced samples is {}\".format(len(X_bal)))\n",
    "\n",
    "    return X_bal, y_bal , ids_bal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle as pkl\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "norm_df = df_bin\n",
    "\n",
    "norm_df.idade = scaler.fit_transform(df_bin.idade.values.reshape((-1, 1)))\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "X = norm_df.drop(columns=[\"transplante_bin\", \"obito_bin\",\"id\"])\n",
    "ids = norm_df[[\"id\"]].values\n",
    "y = norm_df[[\"transplante_bin\"]].values\n",
    "\n",
    "X_balanced, y_balanced, ids_balanced = balance(X,y,ids)\n",
    "\n",
    "X_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X_balanced, y_balanced, ids_balanced)):\n",
    "\n",
    "    fold_dict = {\n",
    "        \"X_train\": X_balanced.iloc[train_index, :],\n",
    "        \"X_test\": X_balanced.iloc[test_index, :],\n",
    "        \"y_train\": y_balanced[train_index],\n",
    "        \"y_test\": y_balanced[test_index],\n",
    "        \"ids_train\": ids_balanced[train_index],\n",
    "        \"ids_test\": ids_balanced[test_index],\n",
    "    }\n",
    "\n",
    "    with open(\"folds/fold_{}.pkl\".format(i), \"wb\") as f:\n",
    "        pkl.dump(fold_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN model creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel(\"INFO\")\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from keras import backend as K\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_builder:\n",
    "    @classmethod\n",
    "    def add_layer(cls, neurons, ac_function, mlp):\n",
    "        mlp = tf.keras.layers.Dense(units=neurons, activation=ac_function)(mlp)\n",
    "        mlp = tf.keras.layers.Dropout(0.2)(mlp)\n",
    "        return mlp\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers,\n",
    "        ac_function,\n",
    "        input_sequence_length,\n",
    "        batch,\n",
    "        iterr,\n",
    "        learningRate,\n",
    "        beta1,\n",
    "        beta2,\n",
    "        epocas,\n",
    "        output_function=\"linear\",\n",
    "        shuffle=True,\n",
    "    ):\n",
    "        self.layers = layers\n",
    "        self.ac_function = ac_function\n",
    "        self.input_sequence_length = input_sequence_length\n",
    "        self.batch = batch\n",
    "        self.iterr = iterr\n",
    "        self.learningRate = learningRate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epocas = epocas\n",
    "        self.shuffle = shuffle\n",
    "        self.output_function = output_function\n",
    "\n",
    "    def fit_predict(self, X_train, y_train, X_test, struct_name, val_split=0.1):\n",
    "        # print(input_sequence_length)\n",
    "        input_layer = tf.keras.layers.Input(shape=(self.input_sequence_length,))\n",
    "\n",
    "        mlp = input_layer  # Inicializar o modelo com a camada de entrada\n",
    "\n",
    "        for neurons in self.layers:\n",
    "            self.add_layer(neurons, self.ac_function, mlp)\n",
    "\n",
    "        # Camada de sa√≠da com output_dim=1\n",
    "\n",
    "        output_function = tf.keras.layers.Dense(1, activation=self.output_function)(mlp)\n",
    "\n",
    "        # Definir o modelo\n",
    "        mlp_model = tf.keras.Model(inputs=input_layer, outputs=output_function)\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.learningRate, beta_1=self.beta1, beta_2=self.beta2\n",
    "        )\n",
    "\n",
    "        # Compilar o modelo\n",
    "        mlp_model.compile(loss=\"binary_crossentropy\", optimizer=opt)\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=val_split, shuffle=self.shuffle, stratify=None\n",
    "        )\n",
    "\n",
    "        early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=self.iterr,\n",
    "            verbose=0,\n",
    "            min_delta=0.001,\n",
    "            mode=\"min\",\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "\n",
    "        start_fit_time = time.time()\n",
    "\n",
    "        history = mlp_model.fit(\n",
    "            x=X_train,\n",
    "            y=y_train,\n",
    "            batch_size=int(self.batch),\n",
    "            epochs=self.epocas,\n",
    "            verbose=0,\n",
    "            callbacks=[early_stopping_callback],\n",
    "            # validation_split=0.0,\n",
    "            validation_data=(X_val, y_val),\n",
    "            shuffle=self.shuffle,\n",
    "            workers=10,\n",
    "            use_multiprocessing=True,\n",
    "        ).history\n",
    "\n",
    "        end_fit_time = time.time()\n",
    "\n",
    "        fit_time = end_fit_time - start_fit_time\n",
    "\n",
    "        # Ploting Model Loss\n",
    "\n",
    "        plt.ioff()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(history[\"loss\"], linewidth=2, label=\"Train\")\n",
    "        plt.plot(history[\"val_loss\"], linewidth=2, label=\"Validation\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Model loss\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "\n",
    "        plt.show(block=True)\n",
    "\n",
    "        fig.savefig(\n",
    "            \"learning_curves/model-loss__\" + struct_name + \"__.png\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        del fig\n",
    "\n",
    "        start_predict_time = time.time()\n",
    "\n",
    "        output = [value[0] for value in mlp_model.predict(X_test, workers=10, use_multiprocessing=True)]\n",
    "\n",
    "        end_predict_time = time.time()\n",
    "\n",
    "        predict_time = end_predict_time - start_predict_time\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "        return output, fit_time, predict_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "hidden_layer_sizes = [\n",
    "    (3, 3),\n",
    "    (5, 5),\n",
    "    (10, 10),\n",
    "    (3, 3, 3),\n",
    "    (5, 5, 5),\n",
    "    (10, 10, 10),\n",
    "]\n",
    "activation = [\n",
    "    \"sigmoid\",\n",
    "    \"tanh\",\n",
    "    \"linear\",\n",
    "    \"relu\",\n",
    "    \"softmax\",\n",
    "    # \"leaky_relu\",\n",
    "    # \"softplus\",\n",
    "    # \"softsign\",\n",
    "    # \"elu\",\n",
    "]\n",
    "\n",
    "output_function_list = [\n",
    "    \"sigmoid\",\n",
    "    \"tanh\",\n",
    "    \"linear\",\n",
    "    \"relu\",\n",
    "    \"softmax\",\n",
    "    # \"leaky_relu\",\n",
    "    # \"softplus\",\n",
    "    # \"softsign\",\n",
    "    # \"elu\",\n",
    "]\n",
    "\n",
    "batch_size = [200, 300, 400]\n",
    "n_iter_no_change = [10]\n",
    "learningRates = [\n",
    "    0.001,\n",
    "    0.0001,\n",
    "    # 0.00001,\n",
    "]\n",
    "beta_1_List = [\n",
    "    0.99,\n",
    "    0.9,\n",
    "    0.85,\n",
    "]\n",
    "beta_2_List = [\n",
    "    0.99,\n",
    "    0.9,\n",
    "    0.85,\n",
    "]\n",
    "\n",
    "results_list = glob.glob(\"grid_results/*\")\n",
    "folds = glob.glob(\"folds/*\")\n",
    "\n",
    "\n",
    "for output_function in output_function_list:\n",
    "    for function in activation:\n",
    "        for batch in batch_size:\n",
    "            for iterr in n_iter_no_change:\n",
    "                for learningRate in learningRates:\n",
    "                    for beta1 in beta_1_List:\n",
    "                        for beta2 in beta_2_List:\n",
    "                            for layers in hidden_layer_sizes:\n",
    "                                hp_str = \"grid_results/sem_tp_doador_layers_{}__activation_function_{}__output_function_{}__batch_{}__iter_{}__LearnR_{}__Beta1_{}__Beta2_{}.pkl\".format(\n",
    "                                    layers,\n",
    "                                    function,\n",
    "                                    output_function,\n",
    "                                    batch,\n",
    "                                    iterr,\n",
    "                                    learningRate,\n",
    "                                    beta1,\n",
    "                                    beta2,\n",
    "                                )\n",
    "\n",
    "                                epocas = 150\n",
    "\n",
    "                                if not hp_str in results_list:\n",
    "                                    # try:\n",
    "                                    global_fit_time = []\n",
    "                                    global_predict_time = []\n",
    "                                    model = MLP_builder(\n",
    "                                        layers,\n",
    "                                        function,\n",
    "                                        63,\n",
    "                                        batch,\n",
    "                                        iterr,\n",
    "                                        learningRate,\n",
    "                                        beta1,\n",
    "                                        beta2,\n",
    "                                        epocas=epocas,\n",
    "                                        output_function=output_function,\n",
    "                                    )\n",
    "\n",
    "                                    for i, fold in enumerate(folds):\n",
    "                                        with open(fold, \"rb\") as f:\n",
    "                                            fold_dict = pickle.load(f)\n",
    "\n",
    "                                        X_train = fold_dict[\"X_train\"]\n",
    "                                        y_train = fold_dict[\"y_train\"]\n",
    "\n",
    "                                        X_test = fold_dict[\"X_test\"]\n",
    "                                        y_test = fold_dict[\"y_test\"]\n",
    "                                        ids_test = fold_dict[\"ids_test\"]\n",
    "\n",
    "                                        (\n",
    "                                            output,\n",
    "                                            fit_time,\n",
    "                                            predict_time,\n",
    "                                        ) = model.fit_predict(\n",
    "                                            X_train,\n",
    "                                            y_train,\n",
    "                                            X_test,\n",
    "                                            (\n",
    "                                                hp_str.split(\"/\")[-1]\n",
    "                                                + \"__fold_{}\".format(i)\n",
    "                                            ),\n",
    "                                        )\n",
    "\n",
    "                                        global_fit_time.append(fit_time)\n",
    "                                        global_predict_time.append(predict_time)\n",
    "\n",
    "                                        diff = [\n",
    "                                                    out - tg\n",
    "                                                    for out, tg in zip(output, y_test)\n",
    "                                                ]\n",
    "\n",
    "                                        if i == 0:\n",
    "                                            global_output = output\n",
    "                                            global_target = y_test\n",
    "                                            global_dict = {\n",
    "                                                \"id\": [id for id in ids_test],\n",
    "                                                \"rse\": diff,\n",
    "                                            }\n",
    "\n",
    "                                        else:\n",
    "                                            global_output = np.hstack(\n",
    "                                                (global_output, output)\n",
    "                                            )\n",
    "                                            global_target = np.hstack(\n",
    "                                                (global_target, y_test)\n",
    "                                            )\n",
    "                                            global_dict[\"id\"] += [id for id in ids_test]\n",
    "                                            global_dict[\"rse\"] += diff\n",
    "\n",
    "                                    result = {\n",
    "                                        \"y_predict\": global_output,\n",
    "                                        \"y_test\": global_target,\n",
    "                                        \"global_dict\": global_dict,\n",
    "                                        \"fit_time\": [np.mean(global_fit_time)],\n",
    "                                        \"predict_time\": [np.mean(global_predict_time)],\n",
    "                                    }\n",
    "\n",
    "                                    with open(hp_str, \"wb\") as handle:\n",
    "                                        pkl.dump(result, handle)\n",
    "\n",
    "                                    # except:\n",
    "                                    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, fold in enumerate(folds):\n",
    "    with open(fold, \"rb\") as f:\n",
    "        fold_dict = pickle.load(f)\n",
    "\n",
    "    X_train = fold_dict[\"X_train\"]\n",
    "    y_train = fold_dict[\"y_train\"]\n",
    "    X_test = fold_dict[\"X_test\"]\n",
    "    y_test = fold_dict[\"y_test\"]\n",
    "\n",
    "    X_train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
